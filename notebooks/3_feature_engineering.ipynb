{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing, metrics\n",
    "from typing import Callable\n",
    "from scipy.sparse import hstack\n",
    "from itertools import product\n",
    "\n",
    "from sales_forecasting.utils import timeseries_split, build_submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\".data/data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter redundant columns\n",
    "cols = ['date_block_num', 'shop_id', 'city_id', 'item_id', 'item_category_id', 'general_item_category_id', 'item_price', 'date_month', 'item_cnt_day']\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outliers\n",
    "shp = df.shape[0]\n",
    "outliers_item_price_index = df['item_price'] > 100000\n",
    "outliers_item_cnt_day_index = df['item_cnt_day'] > 1000\n",
    "\n",
    "display(df[outliers_item_price_index])\n",
    "display(df[outliers_item_cnt_day_index])\n",
    "\n",
    "df = df.drop(df[outliers_item_price_index].index)\n",
    "df = df.drop(df[outliers_item_cnt_day_index].index)\n",
    "print(f\"Filtered {shp - df.shape[0]} outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imput missing/wrong values\n",
    "missing_item_price_index = df['item_price'] <= 0\n",
    "mean_imputation = df[(df.shop_id == 32) & (df.item_id == 2973) & (df.date_block_num == 4) & (df.item_price > 0)]['item_price'].mean()\n",
    "df.loc[missing_item_price_index, 'item_price'] = mean_imputation\n",
    "display(df[missing_item_price_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter repeating shops\n",
    "repeating_shops_pairs =  [(0, 57), (1, 58), (10, 11)]\n",
    "\n",
    "for orig, rep in repeating_shops_pairs:\n",
    "    df.loc[df.shop_id == rep, 'shop_id'] = orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set dtypes\n",
    "# dtypes = {\n",
    "#     'date_block_num': 'uint8',\n",
    "#     'shop_id': 'uint8',\n",
    "#     'city_id': 'uint8',\n",
    "#     'item_id': 'uint16',\n",
    "#     'item_category_id': 'uint8',\n",
    "#     'general_item_category_id': 'uint8',\n",
    "#     'item_price': 'float32',\n",
    "#     'date_month': 'uint8',\n",
    "#     'item_cnt_day': 'int32'\n",
    "# }\n",
    "\n",
    "# for column, dtype in dtypes.items():\n",
    "#     df[column] = df[column].astype(dtype) # type: ignore\n",
    "\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data monthly (sum daily sales)\n",
    "base_cols = ['date_block_num', 'shop_id', 'item_id']\n",
    "cols = base_cols + ['city_id', 'item_category_id', 'general_item_category_id', 'date_month']\n",
    "\n",
    "df_agg_monthly = df.copy()\n",
    "df_agg_monthly['item_cnt_day'] = df['item_cnt_day'].fillna(0)\n",
    "df_agg_monthly = df_agg_monthly \\\n",
    "    .groupby(base_cols).agg({\"item_cnt_day\": \"sum\"}) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={\"item_cnt_day\": \"item_cnt_month\"})\n",
    "df_agg_monthly = df_agg_monthly.merge(df[cols].drop_duplicates(), on=base_cols, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.union1d(np.array([1,2,3]), np.array([2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample for month x shop x item where item_cnt_month == 0 (only train data, as test data already covers such combinations)\n",
    "# matrix = []\n",
    "# cols = ['date_block_num','shop_id','item_id']\n",
    "# for i in range(34):\n",
    "#     df_train_month = df[df.date_block_num==i]\n",
    "#     matrix.append(np.array(list(product([i], df_train_month.shop_id.unique(), df_train_month.item_id.unique()))))\n",
    "\n",
    "matrix, shops_cache, items_cache = [], np.array([]), np.array([])\n",
    "cols = ['date_block_num','shop_id','item_id']\n",
    "for i in range(34):\n",
    "    df_train_month = df[df.date_block_num==i]\n",
    "    shops_cache = np.union1d(shops_cache, df_train_month.shop_id.unique())\n",
    "    items_cache = np.union1d(items_cache, df_train_month.item_id.unique())\n",
    "    matrix.append(np.array(list(product([i], shops_cache, items_cache))))\n",
    "    \n",
    "matrix = pd.DataFrame(np.vstack(matrix), columns=cols)\n",
    "matrix = pd.concat([matrix, df[df.date_block_num==34][cols]], ignore_index=True, sort=False)\n",
    "matrix = matrix.sort_values(cols).reset_index(drop=True)\n",
    "\n",
    "df_agg_monthly_oversampled = pd.merge(matrix, df_agg_monthly[cols + ['item_cnt_month']], on=cols, how='left').fillna(0)\n",
    "df_agg_monthly_oversampled = df_agg_monthly_oversampled \\\n",
    "    .merge(df[['shop_id', 'city_id']].drop_duplicates(), on='shop_id', how='left') \\\n",
    "    .merge(df[['item_id', 'item_category_id', 'general_item_category_id']].drop_duplicates(), on='item_id', how='left') \\\n",
    "    .merge(df[['date_block_num', 'date_month']].drop_duplicates(), on='date_block_num', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_monthly_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_monthly_oversampled[(df_agg_monthly_oversampled.shop_id == 2) & (df_agg_monthly_oversampled.item_id.isin([30, 31, 32]))].groupby([\"shop_id\", \"item_id\"]).apply(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(df: pd.DataFrame, plt_rows: int = 10, plt_cols: int = 2) -> None:\n",
    "    fig, ax = plt.subplots(plt_rows, plt_cols, figsize=(20, 30))\n",
    "\n",
    "    group = df[['date_block_num', 'shop_id', 'item_id', 'item_cnt_month']].groupby([\"shop_id\", \"item_id\"])#[(df.shop_id == 2) & (df.item_id.isin([30, 31, 32]))].groupby([\"shop_id\", \"item_id\"])\n",
    "    group_iter = iter(group)\n",
    "\n",
    "    for i in range(plt_rows * plt_cols):\n",
    "        (_, df_group) = next(group_iter, (None, None))\n",
    "\n",
    "        #display(df_group)\n",
    "        \n",
    "        if df_group is None:\n",
    "            break\n",
    "        \n",
    "        df_group = df_group[df_group['date_block_num'] < 34]\n",
    "\n",
    "        row = i // plt_cols\n",
    "        col = i % plt_cols\n",
    "        ax[row, col].plot(df_group['date_block_num'], df_group['item_cnt_month'], label=\"Sales\")\n",
    "        ax[row, col].scatter(df_group[df_group.item_cnt_month != 0]['date_block_num'], df_group[df_group.item_cnt_month != 0]['item_cnt_month'], color='blue')\n",
    "        ax[row, col].set_ylim(-.1, max(df_group['item_cnt_month']) + 1)\n",
    "        ax[row, col].set_title(f\"Shop ID: {df_group.shop_id.iloc[0]}, Item ID: {df_group.item_id.iloc[0]}\")\n",
    "        ax[row, col].set_xlabel('Month')\n",
    "        ax[row, col].set_ylabel('Sales')\n",
    "        ax[row, col].set_xticks(range(0, 34))\n",
    "        ax[row, col].legend()\n",
    "        ax[row, col].grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_timeseries(df_agg_monthly_oversampled, plt_rows=10, plt_cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_name(prefix: str, iter: list[int]):\n",
    "    return [f\"{prefix}_{i}\" for i in iter]\n",
    "\n",
    "def merge_with_oversampled_index(df: pd.DataFrame, oversampled_index: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_features = df.copy()\n",
    "\n",
    "    return pd.merge(df_features, oversampled_index.reset_index(), on=[\"shop_id\", \"item_id\", \"date_block_num\"], how='outer', suffixes=(None, '_y'), indicator=True)\n",
    "\n",
    "def build_month_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_features = df.copy()\n",
    "\n",
    "    df_features['month_sin'] = np.sin(np.pi / 12 * df['date_month'])\n",
    "    df_features['month_cos'] = np.cos(np.pi / 12 * df['date_month'])\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "def build_lagged_features(df: pd.DataFrame, lags: list[int]) -> pd.DataFrame:\n",
    "    df_features = df.copy()\n",
    "\n",
    "    lagged_features = col_name(\"lagged\", lags)\n",
    "\n",
    "    df_features = df_features.sort_values([\"shop_id\", \"item_id\", \"date_block_num\"])\n",
    "    for feat, lag in zip(lagged_features, lags):\n",
    "        df_features[feat] = df_features.groupby([\"shop_id\", \"item_id\"])['item_cnt_month'].shift(lag).bfill()#.fillna(0)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "def build_rolling_features(df: pd.DataFrame, rolling: list[int]) -> pd.DataFrame:\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    assert \"lagged_1\" in df_features.columns, \"lagged_1 column must be present in the dataframe to create roll for past months\"\n",
    "    rolling_features = col_name(\"rolling\", rolling)\n",
    "\n",
    "    df_features = df_features.sort_values([\"shop_id\", \"item_id\", \"date_block_num\"])\n",
    "    for feat, roll in zip(rolling_features, rolling):\n",
    "        df_features[feat] = df_features.groupby([\"shop_id\", \"item_id\"])['lagged_1'].rolling(roll).mean().bfill().reset_index(level=[0,1], drop=True)\n",
    "\n",
    "    return df_features\n",
    "\n",
    "def drop_merged(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    df_features = df_features.drop(df_features[df_features['_merge'] == \"right_only\"].index)\n",
    "    df_features = df_features.drop(columns=['_merge', 'item_cnt_month_y'])\n",
    "    return df_features\n",
    "\n",
    "def build_features(df: pd.DataFrame, lagged_features: list[int], rolling_features: list[int]) -> pd.DataFrame:\n",
    "    df_featurized = df \\\n",
    "        .pipe(build_month_features) \\\n",
    "        .pipe(build_lagged_features, lags=lagged_features) \\\n",
    "        .pipe(build_rolling_features, rolling=rolling_features)\n",
    "    \n",
    "    assert df_featurized.shape[0] == df.shape[0]\n",
    "    \n",
    "    return df_featurized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_features = list(range(1, 4))\n",
    "rolling_features = [3, 6]\n",
    "\n",
    "cols =  {\n",
    "    'cat': ['shop_id', 'item_category_id', 'general_item_category_id', 'city_id',],\n",
    "    'num': ['month_sin', 'month_cos'] + col_name(\"lagged\", lagged_features) + col_name(\"rolling\", rolling_features), #+ col_name(\"item_name_tfidf\", list(range(0, 1000))),\n",
    "    'target': \"item_cnt_month\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = df_agg_monthly_oversampled[(df_agg_monthly_oversampled.shop_id == 0) & (df_agg_monthly_oversampled.item_id == 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = df[(df.shop_id == 0) & (df.item_id == 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).apply(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_monthly_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_historical_features_and_merge(\n",
    "    df_monthly: pd.DataFrame,\n",
    "    df_daily: pd.DataFrame,  \n",
    "    index_cols: list[str],\n",
    "    agg_col: str\n",
    ") -> pd.DataFrame:\n",
    "    cross_cols = index_cols[1:] # drop date_block_num\n",
    "    new_column_name = f\"avg_{'_'.join([x.split('_id')[0] for x in cross_cols])}_{agg_col}\"\n",
    "\n",
    "    avg_index = df_daily.groupby(index_cols).agg({agg_col: \"mean\"}).reset_index().rename(columns={agg_col: new_column_name})\n",
    "    avg_index[f'{new_column_name}_lag_1'] = avg_index.groupby(cross_cols)[new_column_name].shift(1).bfill()\n",
    "    avg_index = avg_index.drop(columns=[new_column_name])\n",
    "\n",
    "    return df_monthly.merge(avg_index, on=index_cols, how='left').ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_monthly_oversampled = df_agg_monthly_oversampled \\\n",
    "    .pipe(aggregate_historical_features_and_merge, df_daily=df, index_cols=['date_block_num', 'shop_id', 'item_id'], agg_col='item_price') \\\n",
    "    .pipe(aggregate_historical_features_and_merge, df_daily=df, index_cols=['date_block_num', 'shop_id', 'item_id'], agg_col='item_cnt_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_monthly_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_shop_item_price_index = df.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg({\"item_price\": \"mean\"}).reset_index().rename(columns={\"item_price\": \"avg_shop_item_price\"})\n",
    "avg_shop_item_price_index['avg_shop_item_price_lag_1'] = avg_shop_item_price_index.groupby([\"shop_id\", \"item_id\"])['avg_shop_item_price'].shift(1).bfill()\n",
    "avg_shop_item_price_index = avg_shop_item_price_index.drop(columns=['avg_shop_item_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_monthly_oversampled.merge(avg_shop_item_price_index, on=[\"date_block_num\", \"shop_id\", \"item_id\"], how='left').ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_item_price_index = dt.groupby([\"date_block_num\", \"item_id\"]).agg({\"item_price\": \"mean\"}).reset_index().rename(columns={\"item_price\": \"avg_item_price\"})\n",
    "avg_item_price_index['avg_item_price_lag_1'] = avg_item_price_index.groupby([\"item_id\"])['avg_item_price'].shift(1).bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_shop_category_price_index = dt.groupby([\"date_block_num\", \"shop_id\", \"item_category_id\"]).agg({\"item_price\": \"mean\"}).reset_index().rename(columns={\"item_price\": \"avg_shop_category_price\"})\n",
    "avg_shop_category_price_index['avg_shop_category_lag_1'] = avg_shop_category_price_index.groupby([\"shop_id\", \"item_category_id\"])['avg_shop_category_price'].shift(1).bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_category_price_index = dt.groupby([\"date_block_num\", \"item_category_id\"]).agg({\"item_price\": \"mean\"}).reset_index().rename(columns={\"item_price\": \"avg_category_price\"})\n",
    "avg_category_price_index['avg_category_lag_1'] = avg_category_price_index.groupby([\"item_category_id\"])['avg_category_price'].shift(1).bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_shop_item_cnt_index = dt.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg({\"item_cnt_day\": \"mean\"}).reset_index().rename(columns={\"item_cnt_day\": \"avg_shop_item_cnt\"})\n",
    "avg_shop_item_cnt_index['avg_shop_item_cnt_lag_1'] = avg_shop_item_cnt_index.groupby([\"shop_id\", \"item_id\"])['avg_shop_item_cnt'].shift(1).bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_item_cnt_index = dt.groupby([\"date_block_num\", \"item_id\"]).agg({\"item_cnt_day\": \"mean\"}).reset_index().rename(columns={\"item_cnt_day\": \"avg_item_cnt\"})\n",
    "avg_item_cnt_index['avg_item_cnt_lag_1'] = avg_item_cnt_index.groupby([\"item_id\"])['avg_item_cnt'].shift(1).bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_shop_category_cnt_index = dt.groupby([\"date_block_num\", \"shop_id\", \"item_category_id\"]).agg({\"item_cnt_day\": \"mean\"}).reset_index().rename(columns={\"item_cnt_day\": \"avg_shop_category_cnt\"})\n",
    "avg_shop_category_cnt_index['avg_shop_category_cnt_lag_1'] = avg_shop_category_cnt_index.groupby([\"shop_id\", \"item_category_id\"])['avg_shop_category_cnt'].shift(1).bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_category_cnt_index = dt.groupby([\"date_block_num\", \"item_category_id\"]).agg({\"item_cnt_day\": \"mean\"}).reset_index().rename(columns={\"item_cnt_day\": \"avg_category_cnt\"})\n",
    "avg_category_cnt_index['avg_category_cnt_lag_1'] = avg_category_cnt_index.groupby([\"item_category_id\"])['avg_category_cnt'].shift(1).bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_monthly_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(df):\n",
    "    count = -1\n",
    "    months_since_last_buy = []\n",
    "    for index, row in df.iterrows():\n",
    "        count += 1\n",
    "        months_since_last_buy.append(count)\n",
    "        \n",
    "        if row['item_cnt_month'] > 0:\n",
    "            count = -1  # Reset counter if there's a sale\n",
    "    return months_since_last_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm2['months_since_last_buy_shop_item3'] = p(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_monthly_oversampled.groupby([\"shop_id\", \"item_id\"]).apply(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm2[['date_block_num', 'item_cnt_month', 'months_since_last_buy_shop_item3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm2 = dtm.copy()\n",
    "dtm2['months_since_last_buy_shop_item'] = dtm2[dtm2.item_cnt_month != 0].groupby([\"shop_id\", \"item_id\"])['date_block_num'].diff() - 1\n",
    "dtm2['months_since_last_buy_shop_item2'] = dtm2[dtm2.item_cnt_month == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm2['months_since_last_buy_shop_item2'] = dtm2.months_since_last_buy_shop_item.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt.pipe(build_lagged_features, lags=[1, 2]).pipe(build_rolling_features, rolling=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_features(df_agg_monthly_oversampled, lagged_features, rolling_features).sort_values(by=['shop_id', 'item_id', 'date_block_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_split, test_split = timeseries_split(df_agg_monthly_oversampled, max_month=33, col='date_block_num', continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split.groupby(['shop_id', 'item_id'])['item_cnt_month'].transform(lambda x: x.rolling(window=3).mean())\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(train_split_featurized[cols['num'] + [cols['target']]].corr(), annot=True, fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = cols['target']\n",
    "#train_target, test_target = train_split_featurized[target_col].clip(0, 20), test_split_featurized[target_col].clip(0, 20)\n",
    "train_target, test_target = train_split_featurized[target_col], test_split_featurized[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    train_split_featurized.to_parquet(\".data/train_split_featurized.parquet\")\n",
    "    test_split_featurized.to_parquet(\".data/test_split_featurized.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cols = cols['cat']\n",
    "ohe = preprocessing.OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(train_split_featurized[ohe_cols])\n",
    "\n",
    "X_train_cat, X_test_cat = ohe.transform(train_split_featurized[ohe_cols]), ohe.transform(test_split_featurized[ohe_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = cols['num']\n",
    "\n",
    "X_train_num, X_test_num = train_split_featurized[num_cols], test_split_featurized[num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack([X_train_cat, X_train_num]).tocsr()\n",
    "X_test = hstack([X_test_cat, X_test_num]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def baseline_naive_mean_model(X):\n",
    "#     return train_target.mean().repeat(X.shape[0])\n",
    "\n",
    "# y_pred = baseline_naive_mean_model(X_test)\n",
    "# rmse = metrics.root_mean_squared_error(test_target.values, y_pred)\n",
    "# print(\"Baseline model Test RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_leaves': 91,\n",
    "    'max_depth': 37,\n",
    "    'learning_rate': 0.033470401293385826,\n",
    "    'n_estimators': 1748,\n",
    "    'reg_alpha': 0.6471314252482143,\n",
    "    'reg_lambda': 2.9415585687282055,\n",
    "    'colsample_bytree': 0.3,\n",
    "    'subsample': 0.8,\n",
    "    'min_child_samples': 62,\n",
    "    'random_state': 42\n",
    "}\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "model.fit(X_train, train_target)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = metrics.root_mean_squared_error(test_target.values, y_pred)\n",
    "print(\"LGBM model Test RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, test_split = timeseries_split(df_full, max_month=34, col='date_block_num', continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_featurized, test_split_featurized = build_features(train_split, lagged_features, rolling_features), build_features(test_split, lagged_features, rolling_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = cols['target']\n",
    "# train_target, test_target = train_split_featurized[target_col].clip(0, 20), test_split_featurized[target_col].clip(0, 20)\n",
    "train_target, test_target = train_split_featurized[target_col], test_split_featurized[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cols = cols['cat']\n",
    "ohe = preprocessing.OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(train_split_featurized[ohe_cols])\n",
    "\n",
    "X_train_cat, X_test_cat = ohe.transform(train_split_featurized[ohe_cols]), ohe.transform(test_split_featurized[ohe_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = cols['num']\n",
    "\n",
    "X_train_num, X_test_num = train_split_featurized[num_cols], test_split_featurized[num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack([X_train_cat, X_train_num]).tocsr()\n",
    "X_test = hstack([X_test_cat, X_test_num]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def baseline_naive_mean_model(X):\n",
    "#     return train_target.mean().repeat(X.shape[0])\n",
    "\n",
    "# y_pred = baseline_naive_mean_model(X_test)\n",
    "\n",
    "# evaluation_dataset = test_split[['shop_id', 'item_id']]\n",
    "# evaluation_dataset = evaluation_dataset.assign(item_cnt_month=y_pred)\n",
    "# build_submission_df(evaluation_dataset, save_path=\".data/submission_naive_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(**params)\n",
    "model.fit(X_train, train_target)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.root_mean_squared_error(train_target.values, model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = test_split[['shop_id', 'item_id']]\n",
    "evaluation_dataset = evaluation_dataset.assign(item_cnt_month=y_pred.clip(0, 20))\n",
    "build_submission_df(evaluation_dataset, save_path=\".data/submission_xgb4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 50),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 2000),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n",
    "        'cat_smooth' : trial.suggest_int('min_data_per_groups', 1, 100),\n",
    "        'random_state': 42,\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "    model.fit(X_train, train_split[cols['target']].values)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = metrics.root_mean_squared_error(test_split[cols['target']].values, y_pred)\n",
    "    return rmse\n",
    "\n",
    "# Create a study and optimize the hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Score:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def train_sklearn_model(df, model, cols):\n",
    "    ohe_cols = cols['cat']\n",
    "    ohe = preprocessing.OneHotEncoder(handle_unknown='ignore')\n",
    "    ohe.fit(df[ohe_cols])\n",
    "\n",
    "    train_split, test_split = timeseries_split(df, 33, col='date_block_num', continuous=False)\n",
    "\n",
    "    X_train_cat = ohe.transform(train_split[ohe_cols])\n",
    "    X_test_cat = ohe.transform(test_split[ohe_cols])\n",
    "\n",
    "    num_cols = cols['num']\n",
    "    X_train_num = train_split[num_cols]\n",
    "    X_test_num = test_split[num_cols]\n",
    "\n",
    "    X_train = hstack([X_train_cat, X_train_num]).tocsr()\n",
    "    X_test = hstack([X_test_cat, X_test_num]).tocsr()\n",
    "\n",
    "    y_train = train_split['item_cnt_month_clipped']\n",
    "    y_test = test_split['item_cnt_month_clipped']\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    train_rmse = metrics.root_mean_squared_error(y_train, y_pred_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    test_rmse = metrics.root_mean_squared_error(y_test, y_pred_test)\n",
    "    print(f'Train RMSE: {train_rmse} \\nTest RMSE: {test_rmse}')\n",
    "\n",
    "    return model\n",
    "\n",
    "feature_columns = {\n",
    "    'cat': ['shop_id', 'item_category_id'],\n",
    "    'num': ['month_sin', 'month_cos'] + [f'lagged_{_}' for _ in range(1,8)] + [f'rolling_{_}' for _ in (3, 6, 12, 24)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = train_sklearn_model(df, model=LinearRegression(), cols=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = train_sklearn_model(df, model=MLPRegressor(max_iter=100, hidden_layer_sizes=[256, ], verbose=True), cols=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_transformed = ohe.transform(df_test[['shop_id', 'item_id', 'item_category_id']])\n",
    "test_data = lgb.Dataset(df_test_transformed)\n",
    "df_test_predictions = df_test.assign(item_cnt_month=gbm.predict(df_test_transformed, num_iteration=gbm.best_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_n_last(x: np.ndarray, n: int = 3) -> float:\n",
    "    return x[-min(n, x.size):].mean()\n",
    "\n",
    "def aggregate_monthly(df: pd.DataFrame, agg_fn: Callable, agg_scope_name: str) -> pd.DataFrame:\n",
    "    new_cols = [f'prev_item_price_agg__{agg_scope_name}', f'is_prev_item_price__{agg_scope_name}']\n",
    "    #display(df)\n",
    "    months_sales = df['date_block_num'].unique()\n",
    "    #print(months_sales)\n",
    "    mapping = {months_sales[0]: np.nan}\n",
    "    for i in range(1, len(months_sales)):\n",
    "        i_date_block_num = months_sales[i]\n",
    "        #display(df[df['date_block_num'] < i_date_block_num])\n",
    "        mapping[i_date_block_num] = agg_fn(df[df['date_block_num'] < i_date_block_num]['item_price'])\n",
    "\n",
    "    df[new_cols[0]] = df['date_block_num'].map(mapping)\n",
    "    df[new_cols[1]] = df['date_block_num'].map({k: int(np.isnan(v)) for k,v in mapping.items()})\n",
    "\n",
    "    display()\n",
    "\n",
    "    return df[[\"date_block_num\", *new_cols]].reset_index(drop=True)\n",
    "\n",
    "def build_monthly_item_price_features(df: pd.DataFrame, agg_cols: list[str], agg_fun: Callable) -> pd.DataFrame:\n",
    "    agg_scope_name = {\"shop_id\": \"local\", \"item_id\": \"global\"}[agg_cols[0]]\n",
    "    new_features = df.groupby(agg_cols) \\\n",
    "        .apply(aggregate_monthly, agg_fn=agg_fun, agg_scope_name=agg_scope_name) \\\n",
    "        .reset_index(level=-1, drop=True).reset_index() \\\n",
    "        .drop_duplicates()\n",
    "    return pd.merge(df, new_features, on=[*agg_cols, \"date_block_num\"], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_shop_5 = df_train[df_train.shop_id < 10]\n",
    "# df_shop_5 = build_monthly_item_price_features(df_shop_5, agg_cols=[\"shop_id\", \"item_id\"], agg_fun=np.mean)\n",
    "# df_shop_5 = build_monthly_item_price_features(df_shop_5, agg_cols=[\"item_id\"], agg_fun=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shop_price_feat = build_monthly_item_price_features(df_train, agg_cols=[\"shop_id\", \"item_id\"], agg_fun=np.mean)\n",
    "df_shop_price_feat = build_monthly_item_price_features(df_shop_price_feat, agg_cols=[\"item_id\"], agg_fun=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_previous_price = df_shop_price_feat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_previous_price = feature_store_previous_price[['date_block_num', 'shop_id', 'item_id', 'prev_item_price_agg__local', 'is_prev_item_price__local', 'prev_item_price_agg__global', 'is_prev_item_price__global']] \\\n",
    "    .drop_duplicates() \\\n",
    "    .sort_values(['date_block_num', 'shop_id', 'item_id']) \\\n",
    "    .fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(feature_store_previous_price, on=['date_block_num', 'shop_id', 'item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_previous_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_previous_price[feature_store_previous_price.is_prev_item_price__local == 0]['prev_item_price_agg__local'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
